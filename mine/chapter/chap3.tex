
\rhead{\xiaowuhao\sectionindex\quad基于查询自适应的缩略图自动生成方法}
\section{基于查询自适应的缩略图自动生成方法}
\par 视频缩略图是视频分享网站给用户提供的友好接口，质量高的缩略图能帮助用户快速了解视频内容，吸引用户点击，从而极大提升视频检索系统的检索性能，改善用户检索视频体验。目前各视频分享网站为视频添加缩略图主要有两种方法：其一是人工定制，这种方法生成的缩略图质量图很高，但这个过程需要人工浏览并理解视频内容，从复杂的视频信息中提炼重要信息以生成合适的缩略图，所以非常耗时，为每一个视频人工定制一张缩略图显然是不现实的；其二是随机选择一张视频帧作为缩略图，随机的方法多种多样，可以是视频第一帧或者是视频50$\%$处的视频帧，例如，youtube网站随机选择3张视频帧供视频上传用户选择，这种方法固然效率很高，但是很容易导致视频缩略图质量较低。
\par 本章提出了一种基于查询自适应的视频缩略图生成方法，该方法能感知用户的查询意图，并结合视频的图文内容生成出信息丰富，布局美观，自适应用户查询意图的缩略图，从而让用户能了解视频中其最感兴趣的部分，帮助用户做出正确的相关性判断，提高用户体验。本章内容安排如下：3.1节介绍视频内容结构化方法；3.2节提出了视频图文内容的质量评价模型，从而为视频缩略图选取合适的图文内容；3.3节根据已选取的图文内容生成布局美观的视频缩略图；3.4节展示了视频缩略图的生成结果，并对结果进行评估，验证了本文方法的有效性；最后是本章的总结。
\subsection{视频内容结构化方法}
\par 视频数据一种信息丰富的多媒体数据，由于视频数据的复杂性和无结构性，处理视频数据之前需要对视频数据进行有效的组织，将无序的视频流媒体的数据整理成有序的结构化数据，以便高效快速的分析视频数据。视频结构化是指通过视频分割、特征提取、目标识别等方式提取视频内容信息，根据语义关系对视频内容进行分类组织，形成方便计算机及用户检索和理解的文本信息\upcite{key54}。因此，视频内容结构化对视频数据的处理具有重要的意义。视频内容结构化的方式多种多样，没有一个统一的标准，本文提出了一种基于主题单元分割的视频结构化方法，即先根据视频语义检测是视频主题边界，将视频分割成语义连贯的视频主题单元，这部分工作本文第二章已给出了解决方法，然后提取、挖掘每个主题单元的显著、重要的图文信息，并加以有效的浓缩、组织、归纳。通过本文视频结构化方法，可以是计算机及用户快速，清晰地掌握视频主题脉络，并理解各个主题单元的内容。
\subsubsection{听觉通道内容结构化}
\par 对于视频听觉通道内容，先要把难以处理的音频特征转化为易于处理的文本信息，对于有字幕文件的视频，直接使用字幕文本，对于没有字幕文件的视频，本文先用2.1.2小节方法介绍的微软语音转化工具包将视频语音转化为文本信息。对于语音转换来的文本或者字幕文件，首先浓缩文本信息，提取关键词。表3-1给出了本文对youtube网站视频ID为"40riCqvRoMs"语音文本进行关键词提取的得分最高的部分结果。
\begin{table}[htb]
\centering
\caption{rake算法提取关键词结果}
\begin{tabular}{|c|c|}
\hline
关键词 & 权值 \\
\hline
potentially revolutionary technologies & 9.0\\
\hline
explore unseen frontiers & 9.0\\
\hline
highly connected neurons & 9.0\\
\hline
visual processing apparatus & 8.2\\
\hline
typical neural network & 8.0\\
\hline
worldwide research community & 8.0\\
\hline
neural network & 5.0\\
\hline
vision lab & 4.375\\
\hline
computer vision & 3.91\\
\hline
\end{tabular}
\end{table} 
\par 本文使用Rose\upcite{key53}等人的提出的RAKE（Rapid Automatic Keyword Extraction）算法。rake算法提取的关键词并不是一个单一的单词，有可能是一个短语，并且倾向于较长的短语。RAKE算法首先使用标点符号（如半角的句号、问号、感叹号、逗号等）将一篇文档分成若干分句，然后对于每一个分句，使用停用词作为分隔符将分句分为若干短语，这些短语作为最终提取出的关键词的候选短语。对于每个候选短语，公式3.1给出了候选短语的重要性得分phraseScore计算方法：
\begin{equation}\label{1}
	phraseScore = \sum_{i}^{n}\frac{wordDegree(w_i)}{wordFrequency(w_i)}
\end{equation} 
其中n为候选短语包含单词的个数，wordDegree为单词的度，当与一个单词共现的单词越多，则该单词的度就越大，wordFrequency为单词出现的频率，每个候选短语的得分由组成短语的单词得分累加得到，而单词的得分是单词的“度”与单词出现的频率的比值。上面的步骤对文本信息做了提炼、浓缩，接下来结合本文第二章主题边界检测算法得出的视频主题边界及文本信息的时间戳得出各个主题的关键文本。图3-1展示了视频听觉通道结构化的过程。
\begin{figure}[htb]
    \centerline{\includegraphics[width=5in]{figures/16.eps}}
    \ncaption{视频听觉通道内容结构化过程：(a)字幕文件或者语音转换的文本信息;(b)关键词提取后的文本;(c)最后的结构化文本}        
\end{figure}
\subsubsection{视觉通道内容结构化}
\par 由于网络视频大多由非专业人员拍摄，其拍摄设备和拍摄技术参差不齐，导致视频模糊画面较多，视频质量往往较低，而且视频中如果含有较多运动场景，这些摄像机变焦产生的画面运动常常引起画面的模糊，所以首先要过滤掉这些模糊、昏暗、低质量的图像，只有清晰高质量的帧才能作为组成缩略图的候选帧。本文遇到的图像质量评价问题是一个典型的无参照问题，不适合采用传统图像还原问题与高质量原图对比的方式，本文自动聚焦函数（Autofocusing function）来判定视频帧质量的好坏。自动聚焦函数是显微成像领域计算图像模糊常用的方法，其输出值越小，则图像越模糊，图像质量越差，相反，其输出值越大，则图像越清晰，图像质量越高。自动聚焦函数多种多样，例如归一化方差函数\upcite{key55,key56}，拉普拉斯能量函数，Brenner梯度函数\upcite{key57}等等,其中归一化方差函数的性能是最好的\upcite{key55},所以本文使用归一化方差函数衡量个视频帧的模糊程度，公式3.2给出其计算方法：
\begin{equation}\label{1}
	quality(i)=\frac{1}{H \times W \times \mu_i}\sum_{W}^{x=1}\sum_{H}^{y=1}(S_i(x,y)-\mu_i)^2
\end{equation} 
\begin{equation}\label{1}
	\mu_i=\frac{1}{H \times W}\sum_{W}^{x=1}\sum_{H}^{y=1}
\end{equation} 
其中H和W分别为视频帧的高度和宽度，$S_i(x,y)$为视频帧i像素点坐标为（x,y）的亮度值，即HSV色彩空间S通道的值，$\mu_i$为视频帧i的亮度均值，计算方法如公式3.3所示。本文优先选取自动聚焦函数输出值quality(i)大的视频帧作为视频缩略图视觉内容的候选帧。
\par 过滤掉模糊的低质量的视频帧后，后续的处理过程如图3-2所示，首先对视频帧进行镜头分割，去除掉重复冗余的视频帧，然后利用本文第二章主题边界检测算法得出的视频主题边界及视频帧对应的时间戳将视频帧序列分为若干个主题单元，为了找出每个主体单元最显著的视频帧，本文对每个主题单元内的视频镜头根据其视觉特征相似性进行聚类，选取每个类簇的类中心作为这个主题单元的代表帧。在聚类之前，计算每个镜头的颜色直方图用来表示镜头的视觉内容，颜色直方图计算简单，对于视频拍摄时摄像机视角的小变化不敏感，同时对物体的自身运动也有较好的鲁棒性。本节方法中颜色直方图计算方法为16*4*4(H:16,S:4,V:4)的256维的归一化HSV颜色直方图，之所以将H通道的比重划分的更为细致，是因为HSV颜色空间对应于色度、饱和度和亮度人眼色彩视觉特征3要素，而相对于饱和度、亮度两个分量，色度分量更加影响人类的视觉判断。提取所有镜头特征向量后，使用经典的K-Means聚类算法对这些特征向量进行聚类即可。
\begin{figure}[htb]
    \centerline{\includegraphics[width=5in]{figures/15.eps}}
    \ncaption{视频视觉通道内容结构化过程}        
\end{figure}
\par 视频镜头图像中可能还包含一些非常有价值的文字信息，特别是新闻类、教育类、演讲类视频，如图3-3所示。这些文字信息精炼，具有高度的概括性，直接反映了视频的内容，本文使用OCR技术提取视频帧的文字内容，以便后续处理。
\begin{figure}[htb]
    \centerline{\includegraphics[width=5in]{figures/19.eps}}
    \ncaption{有些视频帧含有丰富的文字信息：(a)bbc新闻视频帧；(b)斯坦福公开课视频帧；(c)TED演讲视频帧}        
\end{figure}
\subsubsection{主题单元内容结构化}
\par 本文提出了一种基于主题单元分割的视频内容结构化方法，首先用本文第二章视频主题分割方法将复杂无结构的视频流数据先分割成一个个语义内容外部独立、内部连贯的主题单元片段，对与每个主题单元片段双通道内容进行提炼、浓缩，提炼过程如图3-4所示，最终主题单元被结构化为一个精炼、具有高度概括性和代表性的图文二元组（I,T）,其中'I'代表图像内容，它是从视频原始帧序列经过质量评估、镜头分割、镜头聚类等步骤选取一帧最有代表性的帧得出的；'T'代表文字内容，文字内容来源于视频语音字幕和图像OCR识别得到的文本信息，然后提取文本关键词得到关键词序列。I,T在时间维度上重合，分别以图像和文字的方式概括着视频主题单元的内容。
\begin{figure}[htb]
    \centerline{\includegraphics[width=5in]{figures/20.eps}}
    \ncaption{视频主题单元结构化过程}        
\end{figure}
\par 最终，本文将视频内容结构化为如图3-5所示，视频有若干个主题单元组成，每一个主题单元都由其代表视频帧和关键词序列图文二元组（I,T）表示，这些图文二元组（I,T）以代表帧和关键词序列的形式概括了主题单元最显著的图文信息，最终被存储在数据库中，以便后续计算的需要。
\begin{figure}[htb]
    \centerline{\includegraphics[width=5in]{figures/18.eps}}
    \ncaption{视频结构表示}        
\end{figure}
\subsection{视频缩略图图文内容提取}
\par 本节介绍视频缩略图图文内容提取方法，图文内容的质量直接决定了缩略图的质量，本文提取缩略图图文内容时主要考虑其代表性和与用户查询意图的相关性。关于代表性，上一节本文将无结构的视频数据结构化为一个个精炼、具有高度概括性和代表性的图文二元组，本节将这些图文二元组作为视频缩略图候选图文内容。本节后续内容将介绍如何在这些候选图文二元组中选取符合用户查询意图的图文内容。
\par 用户在视频分享网站里搜索自己感兴趣的视频时，一个典型的搜索过程如下：用户输入代表自己搜索意图的查询词，网站搜索引擎根据查询词返回相关视频给用户。本文计算用户查询词与视频主题单元图文二元组的相关性，以倒序相关性排序图文二元组，选取排名靠前的图文二元组作为视频缩略图内容。
\par 图像内在的语义比较抽象，虽然Karpathy\upcite{key58}等人基于深度网络建立了图片与语义空间的映射从而描述了图片的语义，而本文希望能在用户浏览的过程中在用户可接受的时间内生成出动态的缩略图，但是这种方法是相对耗时的，显然不能满足本文需求。图文二元组的图文在时间维度上是重合的，因此它们的语义是紧密相连的，所以本文计算图文二元组中关键词序列与查询词的相关性来衡量它们之间的相关度。要解决这个问题，首先要把这些自然语言符号数学化，在自然语言处理领域中通常用词向量表示自然言语中的单词。一种最简单的词向量方式是"one-hot representation"，向量的维度为预料库词典的长度，向量的分量只有一个1，其他全为0。这种方法虽然简单，但缺点也很明显，第一，向量维度冗长，容易造成维度灾难；第二，这种向量有”词汇鸿沟“的问题，不能很好的刻画词与词的相似性。本文采用Hinton\upcite{key59}提出的词向量"Distributed Representation"的表示方法，其基本思想在于将语料库训练成一个向量空间，使得语料库中每一个词都对应向量空间中一个固定长度的向量，当计算两个词之间的相似度时，则根据这两个词对应的向量的距离来判断它们之间的相似性。
\begin{figure}[htb]
    \centerline{\includegraphics[width=5in]{figures/38.eps}}
    \ncaption{Skip-gram Model}        
\end{figure}
\par 本文采用Mikolov\upcite{key60,key61}等人提出的word2vec词向量空间计算方法，word2vec能在百万乃至上亿数量及的语料库上进行高效的训练\upcite{key61}。他们使用名为一个浅层神经网络语言模型“Skip-gram Model”对语料库中的每一个词做向量表示，如图3-6所示，模型分为输入层，投影层，输出层。Skip-gram模型在输入已知的w(t)前提下，预测其上下文w(t-2),w(t-1),w(t+1),w(t+2)。
在训练过程中，使得每个词向量与其上下文对数概率P最大化，公式3.4给出了P的计算方法，给定单词序列$w_1,w_2,...,w_T$，nb(t)是单词$w_t$上下文单词集合，$p(w_j|w_t)$是$w_j$和$w_t$两个单词词向量的层次softmax。
\begin{equation}\label{1}
	P=\frac{1}{T}\sum_{t=1}^{T}\sum_{j\epsilon nb(t)}logp(w_j|w_t)
\end{equation} 
本文以大约18万个视频文本内容作为训练数据，得到语料库中每一个词汇的特征向量，每个向量50维，两个词语义相关度以两词向量余弦距离衡量，表3-2展示了部分单词以上述方法得出的最相关的词汇。
\begin{table}[htb]
\centering
\caption{基于Wordvex最相关词汇挖掘}
\begin{tabular}{|c|c|}
\hline
word & related word \\
\hline
computer &  computers,software,technology,electronic,internet,computing,devices,digital\\
\hline
human &  animal,rights,aids,nature,particular,body,that,causes\\
\hline
apple & blackberry,chips,iphone,microsoft,ipad,ipod,intel,software \\
\hline
movie & movies,film,films,comedy,hollywood,drama,sequel,animated\\
\hline
network & network,networks,cable,channel,channels,internet,television,media,satellite \\
\hline
smile & grin,smiles,eyes,smiling,laugh,touch,gentle,smirk,tears,whisper \\
\hline
music & musical,dance,songs,recording,folk,studio,contemporary,artists \\
\hline
history & historical,tradition,great,career,ever,life,greatest,literature,decades,origins \\
\hline
language & languages,word,spoken,vocabulary,translation,arabic,refers,dialect\\
\hline
frog & snake,toad,monkey,spider,lizard,spiny,orchid,rattlesnake,snakes,salamanders \\
\hline
\end{tabular}
\end{table} 
\par 用户查询词与图文二元组关键词序列相关度计算方法主要思想是将两部分词序列基于word2vex语义相关度做一个映射，如图3-7所示，首先对于用户查询词序列中的每一个词，分别计算其与图文二元组关键词序列中每一个词中的语义相关度，那么语义相关度最大对应的词就是其在图文二元组词序列中的映射词，如图3-7中红色线条所示；同理对视频图文二元组中词序列做同样的操作，得到它们在用户查询词序列中的映射，如图3-7绿色线条所示。将两部分映射词语义相关度的值累加除以总词数就是两个词序列之前的语义相关度。算法2给出了计算用户查询词与图文二元组关键词序列的具体算法流程。
\begin{figure}[htb]
    \centerline{\includegraphics[width=6in]{figures/39.eps}}
    \ncaption{用户查询词与视频关键词序列相关度计算}        
\end{figure}
\begin{algorithm}
  \centering
  \caption{文本相关度衡量算法}  
  \label{alg:Framwork}  
  \begin{algorithmic}[htbp]  
    \Require  
		 用户查询词序列，视频关键词序列序列
    \Ensure  
         文本相关度  
    \Function {$Compute\underline{\hspace{0.5em}} Text\underline{\hspace{0.5em}} Relation$}{String[] text1,String[] text2}
    \State double sum=0;
    \For{$s1\ in\ text1[]$}
    	\State double max=0;
    	\For{$s2\ in\ text2[]$}
    		\State double score=Word2VexScore(s1,s2);
    		\If{$score > max$}
    			\State $max \gets score$
    		\EndIf
    	\EndFor
    	\State $sum \gets sum+max$
    \EndFor
    
    \For{$s1\ in\ text2[]$}
    	\State double max=0;
    	\For{$s2\ in\ text1[]$}
    		\State double score=Word2VexScore(s1,s2);
    		\If{$score > max$}
    			\State $max \gets score$
    		\EndIf
    	\EndFor
    	\State $sum \gets sum+max$
    \EndFor
   	\State $result \gets \frac{sum}{(text1.length+text2.lengtg)}$
   	\State \Return result
	\EndFunction
  \end{algorithmic}  
\end{algorithm} 
\subsection{视频缩略图图文内容布局方法}
\par 上文基于图文内容质量，代表性，与用户查询词的语义相关性，从视频图文内容中选出了最合适的图文二元组作为缩略图的候选图文内容，本节介绍缩略图图文二元组布局方法，将这些候选图文内容合理的组织，从而生成美观的缩略图。
\par 在生成缩略图之前，本文首先采用Song\upcite{key3}等人的方法检测候选图像显著性区域，该方法将图像的空间布局结构特征化为图像边界，并且结合多个底层线索优化图像边界，从而得到圆润的显著性灰度图，接着确定一个自适应阈值将图像二值化，最后通过检测轮廓得到图像显著性区域，本文选择面积最大的轮廓作为图像显著性区域。
\subsection{实验结果及评估}

\subsection{本章小结}

此处省略N个字…… 